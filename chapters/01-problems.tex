\chapter{Remarks on analogy}\label{chap:problems}
\largerpage 
Analogy can be defined in many ways, and it can be ascribed to various kinds of processes. The literature on analogy is vast and covers all sorts of phenomena and domains. Most work on it focuses on phenomena that are not directly relevant to the overall question of this book, but which are related in some way or another. In linguistics, the term \textit{analogy} is usually employed whenever a process makes reference to direct comparison of surface items without making use of general rules, or when phonological or semantic similarities are involved, which are not easily captured as categorical generalizations. However, as a concept, analogy is rather fuzzy, and has no precise or unique definition. In the following subsections, I briefly mention some of the different phenomena for which the term analogy has been used, and in the final section of this chapter I focus on the actual kind of systems I will address in the present book.

Making justice to the history of analogy in linguistics would require a book (or several) of its own. Extensive discussions of the development of analogy as a concept in linguistics can be found in  \textcite{Anttila.1977}, \textcite{Rainer.2013} and, most extensively, \textcite{Itkonen.2005}.

\section{The many meanings of analogy}

\subsection{Single case analogy}

The simplest form of analogy is a similarity relation between two single items that plays a certain role in triggering or blocking a phonological or morphological process. An example of this type of analogy has been proposed to explain unpredictable new coinages and neologisms that make use of unproductive morphemes or non-morphemes (\citealt[195]{Motsch.1977}, see also \citealt{Butterworth.1983}). In such cases, a newly coined form does not make use of any derivational morphological process but is directly built on the basis of some existing form instead. \textcite[89]{Booij.2010} cites the examples in \REF{exe-dutch-analogies}:

\il{Dutch}

\begin{exe}
    \ex \label{exe-dutch-analogies}
    \begin{xlist}
        \ex
        \begin{tabular}[t]{lcl}
          angst-haas           & $\rightarrow$ & paniek-haas                  \\
          fear-hare            &               & panic-hare                   \\
          `terrified person'   & $\rightarrow$ & `panicky person'             \\
        \end{tabular}
        \ex
        \begin{tabular}[t]{lcl}
          moeder-taal          & $\rightarrow$ & vader-taal                   \\
          mother-language      &               & father-language              \\
          `native language'    & $\rightarrow$ & `father's native language'  \\
        \end{tabular}
        \ex
        \begin{tabular}[t]{lcl}
          hand-vaardig         & $\rightarrow$ & muis-vaardig                 \\
          hand-able            &               & mouse-able                   \\
          `with manual skills' & $\rightarrow$ & `with mouse-handling skills' \\
        \end{tabular}
    \end{xlist}
\end{exe}

In these three cases, the item \textit{haas} `hare', \textit{taal} `language' and \textit{vaardig} `able' are not derivational morphemes and cannot productively be used in other combinations. These are direct analogical formations because the new coinage is built from an existing compound. Various examples that follow similar processes can be found in other languages as well as can be seen in \REF{exe-germ-en1}-\REF{blends-spanish}:

\il{German}
\il{English}
\il{Spanish}

\begin{exe}
    \ex \label{exe-germ-en1} German\\
    \begin{tabular}[t]{@{}lcl}
      Früh-stück & $\rightarrow$& Spät-stück\\
      early-piece &&late-piece\\
      `breakfast' &$\rightarrow$& `late breakfast'\\
    \end{tabular}
    \ex English\\
    handicaped \& capable  $\rightarrow$ handicapable

    \ex \label{blends-spanish} Spanish\\
    \gll perfumería  + super $\rightarrow$ superfumería \\
    {perfume store} {} very $\rightarrow$ {`large perfume store'}\\

\end{exe}


These are single case analogies because they are single formations based on the similarity to one or two words and not assumed to be a systematic (and predictable) mechanism of the language. This kind of process is not predictably productive, and there are no generalizations about when or where it can apply, but the process seems to be constantly available to speakers. 

Within the rubric of single case analogies, there are multiple kinds of processes \autocite[278]{Anderson.2015}. Some of these are: blending, where two words are joint together to form a a new word \textit{breakfast} + \textit{lunch} $\rightarrow$ \textit{brunch} (also the examples in \REF{blends-spanish}); back formation, where a new base is created for what appears to be a derived form, like the creation of the verb \textit{edit} from the older noun \textit{editor} (compare however \citealt{vanMarle.1985} and \citealt{Becker.1993}); folks etymology, where speakers infer the wrong etymology of a word based on analogy to another word. One such example is the word \textit{vaga\emph{bundo}} `homeless person' in Spanish which is often thought to come from \textit{vagar} `walk aimlessly' and \textit{mundo} `world' and has lead people to think it should be \textit{vaga\emph{m}undo}; affix-based analogy \autocite{Kilani-Schoch.2005}, where an apparent base--affix is extended to new contexts like in the French \textit{aterrir} `to land', from \textit{terre} `earth' $\rightarrow$ \textit{amerrir} `to land on the sea', from \textit{mer} `sea' $\rightarrow$ \textit{alunir} `to land on the moon', from \textit{lune} `moon'.\footnote{The same phenomenon is also found in Spanish with \textit{aterrizar} `to land on earth', \textit{alunizar} `to land on the moon', etc.} Although there are clear differences between these processes, these cases of analogy are all based on individual specific items and do not really involve abstraction across categories.

In language change we also find examples of single case analogies, where the existence of a form prevents another form from following its expected path or, occasionally, leads to unexpected change \autocite{Bauer.2003}. \textcite[276]{Anderson.2015} describes this kind of phenomenon as: ``where the regular continuation of some form would be expected to undergo some re-shaping by sound change, but instead it is found to have been re-made to conform to some structural pattern. This is what we usually mean by ``Analogy''''. \textcite{Rainer.2013} cites an example from the history of Spanish. A regular vowel change that happened between Latin and Spanish is the lowering of /ĭ/ to /e/. Some examples of this change can be seen in \REF{exe-change-lat}:

\il{Latin}

\begin{exe}
    \ex \label{exe-change-lat}
    \begin{xlist}
        \ex pĭlum $\rightarrow$ pelo `hair'
        \ex ĭstum $\rightarrow$ esto `this'
    \end{xlist}
\end{exe}

According to this phonological rule, from the \textit{lat}. \textit{sin\emph{ĭ}strum} `left' the expected Spanish form would be \textit{sin\emph{e}stro}, but because of analogy with the existing Spanish form \textit{diestro} `right (handed)', it became \textit{sin\emph{ie}stro} `sinister'. This is a single case analogical process at work. Because of semantic and phonological similarities to an existing word, some word fails to undergo a regular phonological change.

A \il{German} related phenomenon is called contamination \autocite[160]{Paul.1995}, which happens when two elements are so semantically similar that a new element with properties of both is created by speakers. As an example Paul mentions the German formation \textit{Erdtoffel} `potato' made out of \textit{Kartoffel} and \textit{Erdapple} (both also meaning `potato'), and \textit{Gemäldnis} `painting' formed from \textit{Bildnis} `portrait' and \textit{Gemälde} `painting'. Some of these innovations are sporadic, but some can remain in the language.

Although most studies have almost exclusively focused on morphological and phonological phenomena, there has been some recent work on syntactic analogical change \autocite{DeSmet.2016}. In syntax the idea is the same; a given syntactic construction changes or fails to change, by analogy to some other (usually more frequent) syntactic construction. In syntax, however, it is much harder to be certain that some change was due to analogical relations. A relatively recent (Colombian) Spanish innovation is [\textit{lo más de X$_{\text{adj}}$}] (the most of X, meaning `quite X' shown in \REF{analoy-change-syntax-spanish}:

\il{Spanish}

\newcommand{\spacebr}{\hspaceThis{[}}

\begin{exe}
    \ex \label{analoy-change-syntax-spanish}
    \setlength\tabcolsep{2pt} % default value: 6pt
    \begin{tabular}[t]{l@{}l@{}lcl@{}l@{}lcl@{}l@{}l}
     [ & lo más bonito   & ] & + & [ & de lo más bonito         & ] & $\rightarrow$ & [ & lo más de bonito   & ] \\
       & the more pretty &   &   &   & of the more pretty       &   &               &   & the more of pretty &   \\
       & `the prettiest' &   & + &   & `(one) of the prettiest' &   & $\rightarrow$ &   & `quite pretty'     &   \\
    \end{tabular}
\end{exe}

Here we see that the [lo más de X$_{adj}$] construction is a sort of blend between two different constructions, but has a unique and different meaning from the original constructions.

Comprehensive discussions of the role of analogy in language change and historical linguistics can be found in \textcite{Anttila.2003}, \textcites{Hock.1991, Hock.2003}, \textcite{Trask.1996} and, of special historical relevance, \textcite{Paul.1995}.

Finally, it is important to mention that single case analogy is usually thought of as a cognitive process and not as a description of a system property. Single case analogy is about what speakers do when new forms are coined, single items regularize, or when some predictable phonological change fails to apply in some specific cases. This kind of analogy will not be discussed in this book.

\subsection{Proportional analogies}

\is{proportional analogy|(}

\largerpage 
A different kind of analogy is termed proportional analogy. In its simplest form, proportional analogy involves four elements, such that: \textit{A}:\textit{B}$=$\textit{C}:\textit{X}, \textit{A} is to \textit{B} as \textit{C} is to \textit{X}. The idea here is that we can find \textit{X} by looking at the relation between \textit{A} and \textit{B}. The earliest mention of this kind of analogy is in Aristotle's Poetics:

\begin{quotation}
  By `analogical' I mean where the second term is related to the first as the fourth is to the third; for then the poet will use the fourth to mean the second and vice versa. And sometimes they add the term relative to the one replaced: I mean, for example, the cup is related to Dionysus as the shield is to Ares; so the poet will call the cup `Dionysus' shield' and the shield `Ares' cup'; again old age is to life what evening is to day, and so he will call evening `the old age of the day' or use Empedocles' phrase, and call old age `the evening of life' or `the sunset of life'. \autocite[Chapter III]{Russell.1989}
\end{quotation}


This is a rather old concept, which has also been used in linguistics extensively, most notably in morphology but also in historical linguistics \autocite{Paul.1995}. This kind of analogy is often present in word-based theories of inflection and derivation, where fully inflected forms are related to each other by proportional analogies, instead of operations deriving inflected forms from stems \autocites{Blevins.2006a, Blevins.2008, Blevins.2016a}. \textcite[543]{Blevins.2006a} gives an example from Russian, with the nouns \textit{škola} `school' and \textit{mušcina} `man' in the nominative and accusative as in \REF{exe-analogy-prop}.

\begin{exe}
    \ex \label{exe-analogy-prop} Analogical deduction
    \begin{xlist}
        \ex škol\textit{a}:škol\textit{u} = muščin\textit{a}:X
        \ex X=muščin\textit{u}
    \end{xlist}
\end{exe}

Example \REF{exe-analogy-prop} illustrates that if we know that for the nominative form \textit{škola} there is an accusative form \textit{školu}, then we can infer that for the nominative form \textit{muščina} there will be an accusative form \textit{muščinu}. Word based and exemplar based theories of morphology usually assume that the whole inflectional (and sometimes derivational) system of a language works as a system of analogies between known forms. This also implies that proportional analogy can (and should) be extended to sets. For example, it is not just the relation \textit{škola}-\textit{školu} which determines the relation \textit{muščina}-\textit{muščinu}, it is rather the whole set of nominative-accusative pairs speakers know.

The use of proportional analogies has not been limited to inflectional morphology. There are several proposals for derivational morphology. \textcite{Singh.2003a} propose a model in which derived words and simplex forms are related to each other by proportional analogies and not through morphemes or rules (see \citealt{Singh.2003} for several related papers, also \citealt{Neuvel.2001}). In this approach, formations like: \textit{Marx}:\textit{Marxism}=\-\textit{Lenin}:\textit{Leninism}, are not related by a morpheme \textit{-ism}, but by direct analogies as shown in \REF{analogy-derivation-ism}:

\begin{exe}
    \ex \label{analogy-derivation-ism} /X$_{Name}$/$\rightarrow$/Xizm/
\end{exe}

However, it is not completely clear how this differs from theories like Booij's Construction Morphology \autocite{Booij.2010}, where this exact kind of relation is expressed by a construction in a very similar manner as in \REF{analogy-derivation-ism-cxt}:

\begin{exe}
    \ex \label{analogy-derivation-ism-cxt} [X$_{Name}$-ism] $\leftrightarrow$ [pertaining to SEM(X)]
\end{exe}

\largerpage
Booij (88) suggests the difference between analogy of this kind and constructions is a gradient one, but without a clear formalization it is hard to evaluate this claim. This is a common issue with the use of proportional analogies to model some (or all) of morphology. These proposals are rarely, if ever, properly formalized (a notable exception is \citealt{Beniamine.2017}), and it is not always clear how they differ from rules. From a purely non-cognitive perspective, it is not obvious what it means to say that there are no morphemes or rules, but only analogies between whole forms. The real difference seems to be in the assumptions about mental representation and the need for rich storage of fully inflected forms.

One possible clear distinctive feature of proportional analogy approaches is the existence of bidirectional relations, not usually assumed in other kinds of approaches to morphology. Proportional analogies can usually go in any direction, from any cell in a paradigm to any other cell and from a member of a derivational family to any other of its members. This property also means that there is no need for an arbitrary partition of words into stems and markers/morphemes, but the rules can look at whole words.

The lack of computational implementations of these proposals means that we cannot really evaluate how well word-based models perform at a larger scale. Although very appealing for their simplicity, it is possible that models solely based on proportional analogies cannot capture certain parts of morphology. In the end, we require a precise system that produces the \textit{X} in the analogical equations, and this usually boils down to some sort of phonological rule set. This is not to say that there has been no work on computational implementations of proportional analogies. On the contrary, there is extensive literature on how proportional analogies can be modelled computationally \autocites{Federici.1995b, Fertig.2013, Goldsmith.2007, Lepage.1998, Pirrelli.1994a, Pirrelli.1994, Yvon.1997}. An extensive discussion of this work is not possible, but two issues are worth mentioning. First, most work on computational implementations of analogy focuses on languages like English, Italian or Spanish. This means that it is unclear how well these systems generalize to phenomena not found in Indo-European languages (e.g. phenomena like non-concatenative morphology, tonal processes found in African languages, etc.). Second, well formalized, computational implementations of proportional analogies tend to only cover some part of a language or address some specific task. I am not aware of a computational model of proportional analogies which covers all of derivation and inflection of some language.

\il{Spanish}

\largerpage
A different kind of phenomenon also modeled with proportional analogies is paradigm leveling. Paradigm leveling is the process by which irregular or alternating forms in the paradigm of a verb become homogeneous. A simple recent example is the superlative of \textit{fuerte} `strong' in Spanish. The original form in 19th century Spanish was \textit{f\emph{o}rtísimo} `very strong', but it eventually turned into \textit{f\emph{ue}rtísimo} during the 20th century. The idea is that proportional analogies with \textit{bueno}:\textit{buenísimo} `good',\footnote{The form \textit{bonísimo} existed until around the 19th century. The assumption is that this form also regularized on the basis of other analogies at the time.} \textit{puerco}:\textit{puerquísimo} `dirty', etc., would cause the change.

A generalization of this kind of process can be seen in the development of paradigm uniformity in language change (see \textcite{Albright.2008} for a review). \textcite[144]{Albright.2008} gives the example of the \textit{eu} $\sim$ \textit{ie} alternations in New High German in Table \REF{tab:exe-change-nhg}:\footnote{As marked by \textcite[144]{Albright.2008}, in the example \emph{>} represents a regular sound change while $\Rightarrow$ represents a form that has been replaced by an analogical process.}

\il{German}

\begin{table}
    \small
    \caption{Middle High German to Early New High German}\label{tab:exe-change-nhg}
    \begin{tabular}[t]{llclcl}
      \lsptoprule
      `to fly' & \multicolumn{2}{l}{Middle High German} & \multicolumn{2}{l}{Early New High German} & New High German                            \\
      \midrule
      \textsc{1sg}      & vliuge                                 & >                                         & fleuge  & $\Rightarrow$ & fl\textit{ie}ge  \\
      \textsc{2sg}      & vliugest                               & >                                         & fleugst & $\Rightarrow$ & fl\textit{ie}gst \\
      \textsc{3sg}      & vliuget                                & >                                         & fleugt  & $\Rightarrow$ & fl\textit{ie}gt  \\
      \textsc{1pl}      & vliegen                                & >                                         & fliegen & >             & fliegen          \\
      \textsc{2pl}      & vlieget                                & >                                         & fliegt  & >             & fliegt           \\
      \textsc{3pl}      & vliegen                                & >                                         & fliegen & >             & fliegen          \\
      \lspbottomrule
    \end{tabular}
\end{table}

The singular and plural forms had different diphthongs in MHG and ENHG, but in the change to NHG the singular and plural stems became identical. The claim is that because of an analogical process with the rest of the paradigm, the \textit{eu} forms for the singular cells of the paradigm were replaced by \textit{ie} forms to make the paradigm more uniform. This goes beyond single case analogies, but it can still be seen as regularization product of proportional analogies in the sense that the leveling increases the scope of a proportional analogy, making it more useful for speakers.

Proportional analogies are not really a process. Unlike the kinds of analogies discussed in the previous subsection, proportional analogies hold independently of speakers and cognitive processes. Proportional analogies hold, for example, for morphological paradigms of dead languages no longer spoken. But proportional analogies can motivate a leveling process in a paradigm, as with the examples in \REF{tab:exe-change-nhg}.

\is{proportional analogy|)}

\subsection{Analogical classifiers}
  
A superficially similar, but distinct type of analogy is what I will call analogical classifiers. Analogical classifiers are assumed to be responsible for disambiguating between two alternatives for some lexical item. Languages often exhibit instances where a given lexeme has to be assigned to a certain category or class, or must receive some feature, but this assignment does not directly follow from other morphosyntactic properties of said lexemes. In such cases, speakers are faced with a choice between two or more categories (or processes or features or classes, etc.) that could apply to this item and they must chose from several alternatives. Since speakers do make a choice, and usually there is agreement about what the right choice is, there must be a mechanism in place that disambiguates between the alternatives. This mechanism is \textit{analogical} if it is based on similarity relations between the item that needs to be classified and other items for which class assignment is known. This is the type of analogy I will focus on in the remainder of this book.

The previous sections showed that analogy is sometimes understood as a process speakers use, which is different in the case of analogical classifiers. Here, we do not deal with a process, but a system of relations. As we will see, analogical classifiers can be implemented with the help of various techniques, but this does not mean that the techniques we use to build analogical classifiers have a direct relation to what speakers do. There is so far no answer to this question, and I will not attempt to answer it here.

Analogical classifiers are a relatively popular area of research among both formal and cognitive linguists. The role of phonological conditions on morphological processes and allomorphs has been acknowledged for quite some time \autocites{Kurylowicz.1945, Bybee.1982, Carstairs.1990} as well as the role of semantic factors \autocite{Malkiel.1988} on similar processes. This is usually known in generative grammar as phonologically conditioned allomorphy \autocite{Nevins.2011a} and in usage-based and cognitive linguistics as analogy \autocite{Bybee.1982}. Despite some apparent terminological disagreements, and despite the fact both communities tend to ignore each other, phonologically conditioned allomorphy and analogy (in the sense of analogical classifiers) are not different kinds of phenomena. In both cases, we are dealing with alternations between multiple alternatives, which are resolved on the basis of phonological and semantic factors.

Analogy as a classifier lies in strong opposition to proportional analogies, however. As explained in the previous subsection, according to a model of proportional analogies, given some form $C$ for which we want to find a corresponding $X$, we infer $X$ by looking at items $A$ similar to $C$ for which we know $B$. This approach tries to avoid an abstraction step, namely the use of classes.
 
Given the basic proportional analogy formula \textit{A:B=C:X}, the association between $A$ and $B$ is direct and thus the association between $C$ and $X$ must also be direct. But this does not need to be the case, the association between $A$ and $B$ can be mediated by an intermediate abstract feature. To make things more clear we look at some concrete examples. Tables \ref{tab:a-class-gothic}--\ref{tab:jo-class-gothic} show the inflection classes \textit{-a}, \textit{-ja},\textit{-o} and \textit{-jo} for Gothic nouns \autocite{Braune.1895}.\footnote{In class \textit{-ja} \textit{/ei/} can contract to \textit{/ji/} on long stems.}
   

\il{Gothic|(}

\begin{table}[h]
    \centering
    \caption{Gothic \textit{-a} declension class}\label{tab:a-class-gothic}
    \begin{tabular}{lllllllll}
      \lsptoprule
      & \multicolumn{4}{c}{`day'} & \multicolumn{4}{c}{`bread'}\\
      & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural} & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural} \\
      \midrule

      \textsc{nom} & dags  & -s           & dagōs  & -ōs  & hlaifs  & -s           & hlaibōs  & -ōs  \\
      \textsc{acc} & dag   & -$\emptyset$ & dagans & -ans & hlaif   & -$\emptyset$ & hlaibans & -ans \\
      \textsc{gen} & dagis & -is          & dagē   & -ē   & hlaibis & -is          & hlaibē   & -ē   \\
      \textsc{dat} & daga  & -a           & dagam  & -am  & hlaiba  & -a           & hlaibam  & -am  \\
      \lspbottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
    \centering
      \caption{Gothic \textit{-ja} declension class}\label{tab:ja-class-gothic}
  \begin{tabular}{lllllllll}
    \lsptoprule
    & \multicolumn{4}{c}{`army'} & \multicolumn{4}{c}{`herdsman'} \\
    & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural}   & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural} \\

    \midrule
    \textsc{nom} & harjis & -jis & harjōs  & -jōs  & haírdeis & -eis & haírdjōs  & -jōs  \\
    \textsc{acc} & hari   & -i   & harjans & -jans & haírdi   & -i   & haírdjans & -jans \\
    \textsc{gen} & harjis & -jis & harjē   & -jē   & haírdeis & -eis & haírdjē   & -jē   \\
    \textsc{dat} & harja  & -ja  & harjam  & -jam  & haírdja  & -ja  & haírdjam  & -jam  \\
    \lspbottomrule
  \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Gothic \textit{-o} declension class}\label{tab:o-class-gothic}
    \begin{tabular}{lllll}
      \lsptoprule
      & \multicolumn{4}{c}{`gift'}                   \\
      & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural} \\
      \midrule
      \textsc{nom} & giba                         & -a  & gibōs & -ōs          \\
      \textsc{acc} & giba                         & -a  & gibōs & -ōs          \\
      \textsc{gen} & gibōs                        & -ōs & gibō  & -ō           \\
      \textsc{dat} & gibái                        & -ái & gibōm & -ōm          \\
      \lspbottomrule
    \end{tabular}
\end{table}

\clearpage 

\begin{table}[t]
  \centering
  \caption{Gothic \textit{-jo} declension class}\label{tab:jo-class-gothic}
  \begin{tabular}{lllll}
    \lsptoprule
        & \multicolumn{4}{c}{`band'}                                \\
        & \multicolumn{2}{c}{Singular} & \multicolumn{2}{c}{Plural} \\
    \midrule
    \textsc{nom} & bandi                        & -i   & bandjōs & -jōs      \\
    \textsc{acc} & bandja                       & -ja  & bandjōs & -jōs      \\
    \textsc{gen} & bandjōs                      & -jōs & bandjō  & -jō       \\
    \textsc{dat} & bandjái                      & -jái & bandjōm & -jōm      \\
    \lspbottomrule
  \end{tabular}
\end{table}

If we only consider these four classes, we can find proportional analogies that help predict most cells. For example, knowing the dative plural form \textit{haírdjam} `herdsman' is enough to know that its genitive plural form must be \textit{haírdjē}. However, some cells are not fully determined. Knowing that \textit{gibōs} `gift' is a nominative plural is not enough for us to determine that the nominative singular should be \textit{giba} and not \textit{gibs}, by analogy with \textit{dagōs} `day'.\footnote{Arguably, in a completely word-based approach there would also be confounding analogies with \textit{bandjōs}.}$^{,}$ \footnote{This situation where a cell in a paradigm only partially helps to predict another cell has been approached from an information theoretic perspective \autocite{MoscosodelPradoMartin.2004a, Ackerman.2013a, Blevins.2013, Ackerman.2016, Bonami.2016}. This approach measures the conditional entropy between cells in a paradigm, and thus quantify how informative different cells are about each other. In this book I pursue a different approach using accuracy measures.}

\largerpage[2]
From the perspective of analogical classifiers, the alternative is that the inflectional class completely determines all cells of the paradigm of any lexeme. The individual cells, in turn, carry information about the inflection class. The distinction might seem trivial, but it requires an important abstraction step. From the analogical classifier perspective, the form \textit{haírdjam} uniquely determines that \textit{haírd} belongs to class \textit{-ja} and, similarly, the form \textit{gibōs} should uniquely determine that \textit{gib} belongs to class \textit{-o}. Examples \REF{exe-proportional-schema} and \REF{exe-classifier-schema} schematically represent how each approach works.

\begin{exe}
    \ex \label{exe-proportional-schema} Proportional analogy
    \begin{xlist}
        \ex harjam:harjē=haírdjam:X
        \ex X=haírdē
    \end{xlist}

    \ex \label{exe-classifier-schema} Analogical classifier:
    \begin{xlist}
        \ex harjam $\in$ \textit{class--ja}
        \ex haírdjam $\in$ \textit{class--ja}
        \ex \textsc{gen.pl}, \textit{class--ja}, haírd=haírdjē
    \end{xlist}
\end{exe}

\il{Gothic|)}

While proportional analogies link forms to forms, analogical classifiers link forms to classes. Nevertheless, both analogical classifiers and proportional analogy models share the core idea that new forms can be generated by making reference to stored forms.

For simple cases like the Gothic examples above, there is empirically no difference between the approaches, and from a complexity perspective the analogical classifier requires extra components. On the other hand, analogical classifiers have certain advantages. The first one is that analogical classifiers are compatible with most, if not all, morphological theories. Meanwhile, models that make use of proportional analogy are usually their own theories of morphology. This means that accepting insights from analogical classifiers does not require giving up on other theoretical concepts (e.g. stems, rules of impoverishment or constructions). Additionally, from a historical perspective, analogical classifiers have been argued to be more accurate in describing linguistic change. According to \textcite[506]{Bybee.2015}, constructions are responsible for licensing actual inflected forms, while analogies are responsible for licensing the combination of the aforementioned schemata with new lexical items: ``given the productive schema [[VERB] + ed ]$_{past}$, a new verb is added to the schematic category and that verb thereby becomes regular'', and it is an analogical classifier which assigns a new verb to this schema. \textcite{Bybee.2015} argue that class assignment `categorization' is more important than pure proportional analogies in many cases of historical development. As an example the authors propose the verbs \textit{strike} and \textit{dig}, which ended up in the class of verbs like \textit{cling}, \textit{swing}, \textit{hang}, etc. even though they do not actually match the schemas that describe this class (see next section for a discussion of this case). The argument is that proportional analogies did not actually take place, but speakers simply assigned these verbs to the \textit{V$\sim$u} class: \textit{swing}$\sim$\textit{swung} (compare however \textcite{DeSmet.2016} and \textcite{Fertig.2013} for alternative views on the matter of analogical regularization).

This sort of change is relatively common. Single regular items might be recategorized as belonging to some irregular class, or irregular items might become regularized. Whenever there is a change in markers it tends to happen across the board, applying to all items of a class. This behaviour of inflection classes seems more compatible with a categorization system where class assignment and morphological realization are independent from each other, than with a system were they are handled by the same process.

All this being said, I will not focus on the distinction between analogical classifiers and proportional analogy models, and although I exclusively focus on analogical classifiers, some of the results from the case studies might also apply to models of proportional analogy.



\subsection{Summing up}

I have discussed three types of analogies that have been proposed in the linguistic literature: single case analogies, proportional analogies and analogical classifiers. Although being very different from each other, these three types of analogy all share the property of being processes or relations which: (i) focus on similarities between groups of items and (ii) allow for very fine-grained generalizations. As already mentioned, I will only discus analogical classifiers in this book. Integrating single case analogy with theories of formal grammar will remain an open problem.

Particularly within morphology and phonology, analogical classifiers (computational and non-computational) have been proposed for a variety of languages: Dutch \autocite{Krott.2001}, English \autocites{Bybee.1982, Arndt-Lappe.2011, Arndt-Lappe.2014}, German \autocites{Hahn.2000, Motsch.1977, Kopcke.1988, Kopcke.1998, Schlucker.2011}, Catalan \autocites{Valles.2004, Saldanya.2005}, French \autocite{Holmes.2004, Lyster.2006, Matthews.2005, Matthews.2010}, Polish \autocite{Czaplicki.2013}, Romanian \autocites{Dinu.2012, Vrabie.1989, Vrabie.2000} Russian \autocite{Kapatsinski.2010, Gouskova.2015}, Spanish \autocites{Afonso.2014, Eddington.2002, Eddington.2004, Eddington.2009, Pountain.2006, Rainer.1993, Rainer.2013, Smead.2000}, Navajo \autocite{Eddington.2006}, Zulu \autocite{OBryan.1974}, as well as more theoretically oriented work \autocites{Skousen.1989, Skousen.2002, Skousen.2013} among many others. It is not possible to discuss all, or even the majority, of these works here. In the following sections, I will address some of the most relevant studies. In addition, the case studies in Part II discuss some of the previous models that have tackled the phenomena in question.

\section{The mechanism for analogy}\label{sec:mechanisms-analogy}

So far I have not discussed what the mechanism for implementing the similarity relations in analogical classifiers actually is. As this is not the most crucial issue for the topic at hand, I will not be concerned with the question of the advantages and disadvantages of the different techniques. I will also not address the question of psycho-linguistic plausibility or mental representation. These are, no doubt, important empirical issues, but they are ultimately tangential to the aim of this book. In this section I will present a brief overview of different systems that have been previously proposed and argue for the method I have chosen for the case studies in Part II.

In the literature there are four types of proposals for what the process behind analogy (understood as analogical classifiers) could be. These are listed in \REF{analogy-types}:

\begin{exe}
    \ex \label{analogy-types}
    \begin{xlist}
        \ex simple, contextual rules;
        \ex schemata;
        \ex multiple-rule systems; and
        \ex computational statistical models% (neural networks and AM)
    \end{xlist}
\end{exe}

Many of the studies that have used one or the other also argued for why the alternatives are inferior or not to be preferred \autocite{Albright.2003, Yaden.2003, Eddington.2000, Gouskova.2015}. I will argue instead that, leaving the point about cognitive representation aside, the systems in \REF{analogy-types} are all more or less the same. The small differences we find between these four approaches are rather minor, and, in principle, one can almost always translate from one to the other.

\subsection{Simple rules}

\is{rule systems|(}
Contextual rules are probably the oldest implementation of analogical classifiers, but they are also not associated with the word \textit{analogy} very often. Contextual rules are commonly found in phonology (\citealt{Chomsky.1968} and \citealt{Goldsmith.2011} among many others), but can be used for pretty much any domain. The format of contextual rules is usually \textsc{p / c}, where \textsc{p} stands for some process and \textsc{c} stands for a given context. Of course, not all uses of contextual rules count as analogical classifiers, but this does not prevent the implementation of an analogical classifier by using contextual rules. We can easily convert the format above into \textsc{c / f}, where \textsc{c} stands for a class and  \textsc{f} for a feature, meaning that if an item has some feature \textsc{f} it then belongs to class \textsc{c}.

Phenomena that can be described in this manner are usually very small (in number of classes) and the generalizations tend to be rather straightforward. One well known example in the literature is the nominative marker in Korean \autocite{Lee.1989, Song.2006}.\footnote{The actual distribution of this particle is more complex than just a nominative marker. See \textcite{Song.2006} for a thorough description of its morphosyntactic properties.} Korean nouns take the nominative marker \textit{-i} after consonants and \textit{-ka} after vowels as seen in \REF{korean-examples}:

\begin{exe}
    \ex \label{korean-examples}
    \begin{xlist}
        \ex mom-i `body.\textsc{nom}'
        \ex kanhowen-i `nurse.\textsc{nom}'
        \ex nay-ka `I.\textsc{nom}'
        \ex k$^h$o-ka `nose.\textsc{nom}'
    \end{xlist}
\end{exe}

Based on grammatical descriptions, there do not seem to be any exceptions to this rule. One could model this behaviour in terms of rules as illustrated in \REF{korean-classes}.\footnote{Alternatively one could define only: \textit{-i / \_C\#} as contextual, and \textit{-ka} as default or the other way around.}

\begin{exe}
    \ex \label{korean-classes}
    \begin{xlist}
        \ex -i / \dots{} C\#
        \ex -ka / \dots{} V\#
    \end{xlist}
\end{exe}

But this is not a classifier. This is rather a morphological process that takes into account the phonological context under which it can apply. To model this phenomenon with an analogical classifier we simply propose two noun inflection classes for Korean: \textit{class--i} and \textit{class--ka}. Nouns belonging to \textit{class--i} take the marker \textit{-i} in the nominative, while nouns that belong to \textit{class--ka} take \textit{-ka} in the nominative. Then, the rules in \REF{korean-classes-2} assign nouns to either class:

\begin{exe}
    \ex \label{korean-classes-2}
    \begin{xlist}
        \ex \textit{class--i} / \dots{} C\#
        \ex \textit{class--ka} / \dots{} V\#
    \end{xlist}
\end{exe}

\largerpage
This might look like we have simply rewritten same statement a different way, but it shows that analogical classifiers can easily handle simple regular cases of phonologically determined allomorphy.
It also shows that simple contextual rules can be used to implement analogical classifiers without difficulty.

Although the Korean example is completely regular, this is rarely the case in phonologically conditioned allomorphy. The seemingly simple plural system in Spanish is a good example to illustrate this. Spanish nouns can end in vowels (\textit{gato} `cat.\textsc{masc}' or consonants (\textit{baúl} `trunk', but not glides. The plural morpheme in Spanish has two main allomorphs: \textit{-s} and \textit{-es}, which are almost always predictable from the final segment of the singular form of the noun, as can be seen in \REF{exe-sp-plurals}:

\begin{exe}
    \ex \label{exe-sp-plurals}
    \begin{xlist}
        \ex \textit{class--s} / \dots{} V\#
        \ex \textit{class--es} / \dots{} C\#
    \end{xlist}
    \ex
    \begin{xlist}
        \ex gatos
        \ex baúles
    \end{xlist}
\end{exe}

However, it is easy to find systematic exceptions to this simple rule. One kind of  exception is found in relatively recent English loanwords: \textit{(e)sticker} -- \textit{(e)stickers} `sticker',\footnote{Since this word is still in its early stages of borrowing there is no established orthography, but the pronunciation is /estiker/.} \textit{snicker} \textit{snickers}, as well as with older French loanwords: \textit{cabaret} -- \textit{cabarets} `cabaret', \textit{carnet} -- \textit{carnets} `ID card'. Less systematic exceptions occur in words with atypical phonotactic patterns such as \textit{ají} `chili peper' or \textit{colibrí} `hummingbird' which can take several different plural forms: \textit{ajís}/\textit{ajíes}/\textit{ajises} and \textit{colibrís}/\textit{colibríes}. These are atypical because Spanish words do not usually end in a stressed /i/, but they are systematic in the sense that other words with this same ending would also allow for at least two different allomorphs (e.g. \textit{manatí} -- \textit{manatís}/\textit{manatíes} `manatee'. This set of additional contexts could also be captured by additional rules:\footnote{One clarification would have to be added regarding additional exceptions like \textit{caset} plural \textit{casetes}/\textit{casets}, where the system seems to have added a more regular plural.}

\begin{exe}
    \ex \begin{xlist}
        \ex \textit{class--es} / \dots{} í\#
        \ex \textit{class--s} / \dots{} et\#
        \ex \textit{class--s} / \dots{} ker\#
    \end{xlist}
\end{exe}

Additional (exception) classes would also be needed for markers like \textit{-ses}: \textit{aji\emph{ses}} `chili pepers', \textit{doce\emph{ses}} `twelves'. What this Spanish example shows is that even apparently simple cases might have some hidden complexity. In the end, however, contextual rules can be used to build a classifier that captures the system.

Phonologically conditioned allomorphy is a well known problem and there are many examples in the literature \autocites{Alber.2009, Anderson.2008, Baptista.2006, Booij.1998, Carstairs.1998, Malkiel.1988, Rubach.2001}, a recent review is given by \textcite{Nevins.2011a}. However, the generative literature has almost exclusively focused on cases where the phonological conditioning is straightforward and can be written as a set of rules or constraints, ignoring those cases where there are no simple rules that can account for the phenomenon.

There are several reasons why phonologically conditioned allomorphy presents difficulties for traditional grammar theories. The main one is that this is a phenomenon which seems to be completely unmotivated and which adds unnecessary complexity to the grammar. The second reason is that many cases do not seem to follow any sort of clear rule pattern (although as we will see, if one looks closely enough, this is not the case). The lack of clear patterns means that the rules in the grammar must make reference to arbitrary features or adhoc constraints.

\is{phonologically conditioned allomorphy}
\is{rule systems|)}

\subsection{Schemata}

\is{schema|(}

The previous subsection showed that Spanish plural formation, although relatively simple, is not uniquely determined by one single rule, but rather by several rules that make reference to the different endings of nouns. With this example in mind, one might ask how specific the phonological environment can be, and how many different possible environments there can be that determine a given alternation. There is no theory-internal or theoretically motivated answer to this question. In principle, the context of a rule could make reference to many segments, and one could have a system with dozens of different contexts. While the formal literature talks about rules, the usage-based literature talks about schemata.

To illustrate this, we will look at the phenomenon probably most often discussed in the literature: irregular verb formation in English. Regular verbs in English build their simple past form adding a \textit{-t/d} marker to the stem. Additionally, there are groups of irregulars which do not follow this pattern. \textcite{Bybee.1982} showed that forms in \REF{ex-past-ew} are not arbitrarily irregular (see also \textcite{Kopcke.1998a} for a comparable analysis of German strong verbs) but that there are schematic properties they all share and that nonce words can be assigned to this conjugation pattern if they are formally similar enough to other existing items. \textcite{Bybee.1982} call these similarity relations a schema. For \REF{ex-past-ew} they propose: /\textit{\dots ow\#}/$\sim$/\textit{\dots uw\#}/, and for \REF{ex-past-nk}: /\textit{\dots ɪ(N)K\#}/$\sim$/\textit{\dots u(N)K\#}/.\footnote{Where \textit{K} stands for a velar and \textit{N} stands for a nasal.}

\begin{exe}
    \ex \label{ex-past-ew}
    \begin{xlist}
        \ex draw -- drew
        \ex blow -- blew
        \ex grow -- grew
        \ex know -- knew
        \ex throw -- threw
    \end{xlist}

    \ex \label{ex-past-nk}
    \begin{xlist}
        \ex stick -- stuck
        \ex sink -- sunk
        \ex swing -- swung
        \ex string -- strung
    \end{xlist}
\end{exe}

One could suggest more detailed schemata (e.g. make reference to the initial consonant cluster structure most verbs in \REF{ex-past-ew} seem to share: \textit{/CL\dots/},\footnote{Where \textit{L} stands for a liquid.}, etc.)

The difference between schemata and rules is not obvious. One factor that has been mentioned as distinguishing schemata from rules (and favouring the former) is that they interact with prototype theory \autocite{Kopcke.1998a}. While rules are blind to what lexical items they apply to, schemata can take into consideration the prototype of a class. In \REF{ex-past-nk}, the prototypes would be \textit{swing} or \textit{string}, and new items will be more or less likely to belong to this same class according to how similar they are to these prototypical items. In a prototype approach to analogy, the analogical relation to the prototype(s) of a class is more important than the relation to non-prototypical items. In such a system, schemata do not need to be completely strict, but specify preferences. They can match items that are not a perfect match, but only partially fit them.

Schemata are usually more specific than rules, and list more phonological material, but this can be emulated equally well by rules. The supposed softness of schemata can also be modelled with either more specific, larger sets of rules, or with rule weights, as in the following section.

\textcite[chapter 11.2--11.3]{Croft.2004} argue that schemata can be output-oriented, i.e. they can specify the specific value of certain output, independently of what the input would be (see also \citealt{Bybee.1995}). In \REF{ex-past-nk}, the output schema would be \textit{[\dots ʌŋ]$_{past}$}. This schema then groups together all verbs that build their past form with /ʌŋ/, independently of what their present form/stem is, and what processes would need to apply to them to form the past form.

It is important to note that output-oriented schemata are a way of generalizing over inflected forms. However, these kinds of schemata are not classifiers. From the schema \textit{[\dots ʌŋ]$_{past}$} one cannot know whether a particular verb inflects according to this schema or not. There needs to be a different mechanism which links the present tense form with the past tense form, or the lexeme with this output schema. Therefore it remains unclear whether this kind of schemata are relevant for analogical classifiers.

The difference between schemata and rules is a subtle one, and it usually has more to do with cognitive representation and performance. Both rules and schemata would need to be formalized before one could establish that they are not equivalent. Currently, there is no way of assessing whether the difference is spurious. In any case, it is always possible to translate a rule-based system to a schema-based system and the other way around. In the end, the use of one or the other seems to be more determined by the theoretical background of the researcher. Formal linguists usually prefer the use of rules, while cognitive and usage-based linguists prefer schemata.

\is{schema|(}

\subsection{Multiple-rule systems}

\is{rule systems!multiple|(}

The generalization of simple rule-based systems is the use of multiple-rule systems. There is no unified theory of how multiple-rule systems (for the purpose of modelling allomorphy) should work. A system could include a specific order of application, follow Panini's principle\footnote{Panini's principle says that in cases where two rules compete with each other, the more specific rule will win the competition \autocite{Zwicky.1986}.}, or be entirely ordering agnostic. One can write rules that only look at endings of words, complete word forms, semantics, etc. Rules can be categorical, assign weights, or be probabilistic.  Since there is no agreement regarding what the properties of these systems should be, I will briefly discuss two cases from the literature.

\subsubsection{Estonian inflectional classes}

An impressive example of classes modelled with multiple rules, is the Estonian inflectional system. There are around 40 inflectional classes for Estonian nouns depending on how one counts main classes and subclasses \autocites{Erelt.1995, Erelt.1997, Murk.1997, Blevins.2008}, and there is no obvious systematic way of predicting the class of a noun. \textcite[242]{Blevins.2008} gives the examples in \tabref{tab:estonian-classes} to illustrate the three main Estonian inflection classes (originally in \citealt{Erelt.2001}).\footnote{The grave accents indicate overlong syllables. The numbers in brackets indicate the inflectional subclass given in \autocite{Erelt.2001}} These three classes in turn can be subdivided into further subclasses.

\il{Estonian}

\begin{table}
    \caption{Main Estonian inflectional classes}\label{tab:estonian-classes}
    \centering
    \begin{tabular}{lllll}
      \lsptoprule
      & \multicolumn{4}{c}{Class I}  \\

      & \textsc{sg}     & \textsc{pl} & \textsc{sg}  & \textsc{pl}          \\
      \midrule
      \textsc{nom}         & maja      & majad   & \`{}lipp  & lipud         \\
      \textsc{gen}         & maja      & majade  & lip       & \`{}lippude   \\
      \textsc{part}        & maja      & majasid & \`{}lippu & \`{}lippusid  \\
      \textsc{illa2/part2} & \`{}majja & maju    & \`{}lippu & lippe         \\
      \midrule
      & \multicolumn{2}{c}{`house' (3)} & \multicolumn{2}{c}{`flag' (20)}\\
      \midrule
      & \multicolumn{2}{c}{Class II} & \multicolumn{2}{c}{Class III}\\
      & \textsc{sg}     & \textsc{pl} & \textsc{sg}  & \textsc{pl}          \\
      \midrule
      \textsc{nom}         & kirik   & kiriku & inimene      & inimesed\\
      \textsc{gen}         & kiriku  & kiriku & inimese      & inimeste\\
      \textsc{part}        & kirikut & kiriku & inimest      & -       \\
      \textsc{illa2/part2} & -       & -      & ini\`{}messe & inimesi \\
      \midrule
       & \multicolumn{2}{c}{`church' (12)} & \multicolumn{2}{c}{`person' (12)} \\
      \lspbottomrule
  \end{tabular}
\end{table}

From the examples in \tabref{tab:estonian-classes} we see that these classes show different markers for most cells. Despite its apparent complexity, the inflectional class of a noun is highly predictable from its phonological shape (with some exceptions). \textcite{Viks.1995} shows a model that can successfully predict the inflectional class of most Estonian nouns (see also \citealt{Viks.1994}). Viks' model consists of a series of handwritten rules that make use of three features: number of syllables, final phonemes of the stem and medial phonemes. Of the final set of 117 rules, 28 alone offer some 73\% coverage, while the remaining 89 offer around 27\% coverage on their own. The total set of rules covers 93\% of nouns\footnote{The coverage does not add up to 100\% because there is some overlap.}. The main point here is not a detailed description of all of Viks' rules, the interesting aspect of this system is that a small set of rules covers a relatively large portion of nouns, while a larger set of rules is there to account for the rest of the system.

As an example we can see the two rules for nouns in \tabref{tab:exe-rules-viks}. In the description of the segments, Viks uses the symbols \textit{c} to indicate any of the consonants: \textit{BDFGHJKLMNPRSÐZÞTV} and capital letters stand for literal letters. The class is a number as defined in \textit{A concise morphological dictionary of Estonian} \autocite{Viks.1992}.\footnote{Notice the class numbers are arbitrary and independent of the rules and rule-ordering.}

\begin{table}[t]
    \centering
    \caption{Rule system according to \textcite{Viks.1992}}\label{tab:exe-rules-viks}
    \begin{tabular}[t]{llllll}
      \lsptoprule
      & n. syllables & final sounds & medial sounds & class & coverage (n. nouns) \\
      \midrule
      a. & 1                   & c            & 0             & 22            & 2612          \\
      b. & 3                   & cUS          & 0             & 11            & 2036          \\
      \lspbottomrule
    \end{tabular}
\end{table}

To decide between the many different rules, \citeauthor{Viks.1995}' (1995) model uses a simple rule-ordering procedure, ``as soon as the first matching rule is found it is implemented regardless of the following ones''. The rules follow an extrinsic order, designed to maximize the accuracy of the system. \citeauthor{Viks.1995}' (1995) model fulfills all characteristics of an analogical classifier: it makes use of phonological properties of lexemes to assign them an inflection class.

\subsubsection{English past tense formation (again)}

\il{English}

A different example of a multiple-rule-based system is discussed by \textcite{Albright.2003}. In this study, the authors compare three possible models for the formation of the past tense in English verbs: (i) a simple rule-based model, (ii) a weighted, multiple-rule-based model, and (3) an analogical model based on work by \textcite{Nosofsky.1990}.

The weighted rule-based model proposed by \textcite{Albright.2003} is based on the minimal generalization algorithm first proposed in \textcite{Albright.1999}. The basic idea of this algorithm is as follows. For a given morphological process that applies to a set of items, the algorithm first tries to generalize across the set of items (in this case past tense formation) and then infer the minimal rules that captures all items. For example, if the algorithm only sees \textit{shine}-\textit{shined} and \textit{consign}-\textit{consigned}, it will make the generalization in \tabref{tab:minima-gen-al}.

 

\begin{table}
    \small
    \caption{Minimal generalization learner} \label{tab:minima-gen-al}
    \begin{tabular}[t]{llllll}
      \lsptoprule
      & change & variable & shared feature & shared segment & change location \\
      \midrule
      a. & $\varnothing{}\rightarrow$ d / & & ʃ & aɪn & \_\_ ]$_{+\text{past}}$ \\
      b. & $\varnothing{}\rightarrow$ d / & kən & s & aɪn & \_\_ ]$_{+\text{past}}$ \\
      c. & $\varnothing{}\rightarrow$ d / & X & $\left[\begin{array}{c}
                                               +\text{strident}\\
                                               +\text{contin}\\
                                               -\text{voice}
                                             \end{array}\right]$ & aɪn & \_\_ ]$_{+\text{past}}$ \\
      \lspbottomrule
    \end{tabular}
\end{table}

The steps in \tabref{tab:minima-gen-al} show how the minimal generalization algorithm works. In the first column, we see the phonological change that needs to be applied to the present tense form, in this case adding a /d/. As to the other columns, in (a) and (b) we see two individual instances of attested past tense forms with their corresponding present tense form. The step in (c) corresponds to the minimal generalization of (a) and (b). It assigns an \textit{X} to the segments which are not common between both forms, generalizes over /ʃ/ and /s/ in terms of their feature representation and keeps the shared segments /aɪn/. This is all within the general context of the operation of forming the past tense.

After this process is iterated, the algorithm arrives at a series of rules, of different degrees of generality, that cover the attested items. Using the accuracy of the rules and their coverage (how many items they apply to), the model then calculates weights for these rules. The weights allow the model to infer degrees of confidence for each rule and to the forms derived from them. This model can thus emulate, to a certain extent, the schemata proposed by \textcite{Bybee.1982}, in that the clusters of similarity like \textit{fling}-\textit{flung}, \textit{sting}-\textit{stung}, \textit{cling}-\textit{clung} can be captured by small rules that specifically apply to them. For these three items, the minimal generalization learner produces the rule: /ɪ/ $\rightarrow$ /ʌ/ / [[-voice] l\_\_ŋ]$\{[+past]\}$. For the larger, more general set that adds \textit{win}, \textit{swing}, \textit{dig}, \textit{spring}, \textit{spin}, \textit{sting}, \textit{wring}, \textit{string}, the model has the more general rule: ɪ $\rightarrow$ ʌ / [XC\_\_[+voice, -continuant]]$\{[+past]\}$. And so on for the other cases. With these sets of rules, Albright and Hayes's model predicts that there should be ``islands of reliability'' in the irregular past tense, where verbs that look alike, by conforming to the context of the rules, will behave according to said rules.

To evaluate their model against the purely analogical model, \textcite{Albright.2003} performed two wug experiments where they asked speakers to produce the past tense of nonce verbs. These words were selected to either belong, or did not belong to the islands of reliability predicted by their model. The authors compared the responses given by the speakers with the probabilities predicted by the three different models. In the end, the multiple-rule-based model outperformed other computational models, including a multiple-rule-based model that did not include weights.

Since \citeauthor{Albright.2003}' (2003) original model works from inflected forms to inflected forms, it is not, in the strict sense, an analogical classifier. However, the minimal generalization learner as a method for inferring rules could easily be deployed in an analogical classifier. An important aspect of \citet{Albright.2003}'s system is that the rules it produces are weighted rules, unlike the rules in \citeauthor{Viks.1994}' (1994) system. This also means that there is no rule-ordering but weight comparison. If two different rules make different predictions for the same input lexeme, the prediction with the highest weight wins. Rule weights correspond, to a certain extent, to the idea of prototypes in the schema-based model. Rules wight stronger weights capture the more prototypical shapes in the system.

\is{rule systems!multiple|)}

\subsection{Neural networks and analogical modelling}

\is{neural network}

Two of the main computational implementations of analogy, and the ones I will focus on in this section, are neural networks and Analogical Modelling (AM).\footnote{Other exemplar-based models have received considerably less attention, see \citet{Matthews.2005} for an overview.} The use of neural networks in linguistics has a relatively long history \autocites{Bechtel.2002, Churchland.1989, McClelland.1986, Rumelhart.1986, Rumelhart.1986a}. The early models were labelled connectionist models and were aimed at explaining much more than just the choice between alternatives. In the second part of this book I will give a more detailed explanation of how neural networks work, but the basic idea of neural networks is that they represent (linguistic) systems in the form of weights between input, hidden and output nodes. In the context of connectionist models, input nodes see the surface linguistic forms, hidden nodes are used by the networks to represent the system in a non-symbolic way and output nodes produce the surface outputs.\footnote{In principle, neural networks simply relate inputs to outputs, with an arbitrary number of intermediate hidden layers. Inputs and outputs can be anything, not just surface linguistic forms.}

Roughly speaking, there are two kinds of neural network implementations. Early connectionist models tried to directly link meaning to form, without any kind of category assignment. That is, in a neural network predicting past tense formation in English, the network would directly learn the past tense forms of verbs and directly produce inflected verbs.
The alternative approach is to train the model to learn categories. Instead of directly learning that the past tense of \textit{fly} is \textit{flew}, the model would learn that \textit{fly} belongs to the class of verbs that form the past tense with a vowel change to \textit{/ew/} (i.e. an analogical classifier).

The framework of AM was initially developed by Skousen \autocites{Skousen.1989, Skousen.2002, Skousen.2013} and has been applied to a variety of different phenomena like gender assignment \autocites{Eddington.2002, Eddington.2004}, compounding \autocite{Arndt-Lappe.2011}, suffix competition \autocite{Arndt-Lappe.2014} and past tense formation \autocite{Derwing.1994}, among others. \textcite[193]{Derwing.1994} summarize the logic behind AM as follows:

\begin{quotation}
to predict behavior for a particular context, we first search for actual examples of that context in an available data base [\dots] and then move outward in the contextual space, looking for nearby examples. In working outward away from the given context, we systematically eliminate variables, thus creating more general contexts called supracontexts. The examples in a supracontext will be accepted as possible analogs only if the examples in that supracontext are homogeneous in behaviour. If more than one outcome is indicated by this search, a random selection is made from among the alternatives provided
\autocite[193]{Derwing.1994}
\end{quotation}

\largerpage
The idea is that the classification of an item is made based on how other similar items are classified. The mathematical implementation is not too important here, what is important is that AM has essentially the same properties as a neural network.\footnote{This should not be taken to mean that both produce exactly the same result, but that the results they produce are very similar.} To be clear, computationally AM and neural networks are very different from each other. The point is that they are conceptually very similar. This point has already been argued by \textcite[289]{Matthews.2005}, who explains that there is no crucial difference between AM and connectionist models, as long as the connectionist model is trained as a classifier:

\begin{quotation}
a [neural] network designed to produce the same category mapping would have exactly the same property [as AM]. Indeed, when a network is constructed to produce just classificatory outputs, its behaviour is almost identical to that produced by AM
\autocite[289]{Matthews.2005}
\end{quotation}

It also follows that other approaches to analogical classifiers do practically the same job. Schemata are a way of measuring and finding groups of items that are surface similar, the same as the weighted rule approach. Even simple context rules like those found in phonology delimit groups of similar items.

\subsection{Analogy or rules}

\is{analogy and rules|(}

The discussion of analogy/similarity systems vs rule-based systems is not new. \textcite{Nosofsky.1989} observed that rules can be used to compute similarity, which in turn would produce analogical systems. The distinction between both kinds of processes is not a simple one. The most explicit treatment of the differences between analogy and rules is given by \textcite{Hahn.1998}. The authors first acknowledge that with the common conception of rules vs analogy (the authors use the term `similarity' ``the best empirical research can do is to test particular models of each kind, not `rules' or `similarity' generally'' (199), but then attempt to provide a clear way of distinguishing between rules an analogy.

They identify two distinctions: (i) absolute vs partial matches, and (ii) relative degree of abstractness of the stored pass elements. Regarding (i) the authors say that:

\begin{quotation}
the antecedent of the rule must be strictly matched, whereas in the similarity comparison matching may be partial. In strict matching, the condition of the rule is either  satisfied  or  not  - no  intermediate  value  is  allowed.  Partial  matching,  in contrast, is a matter of degree - correspondence between representations of novel and  stored  items  can  be  greater  or less \autocite[202]{Hahn.1998}
\end{quotation}

and regarding (ii) that:

\begin{quotation}
  Second,  the  rule  matches  a  representation  of  an  instance [\dots] with a more abstract representation of the antecedent of the rule [\dots], whereas the similarity paradigm matches equally specific representations of new and past items. The antecedent ‘abstracts away’ from the details of the particular instance, focusing on a few key properties \autocite[202]{Hahn.1998}
\end{quotation}

These arguments for distinguishing rules from analogy are unconvincing, however. The argument in (i) only really matters if we can determine, with some independent method, the size of the units that the rules or similarity relations should have. Otherwise, any partial matching process can be emulated with ranked constraints, decision trees, or weighted or ordered rules, as long as these rules are smaller than the larger partial match. So, for example, partial string matching of two strings can be decomposed into categorical matching of their corresponding substrings: given the strings ``aabc'' and ``aabb'', a categorical rule will find a partial match, as long as the rule compares 3 letter substrings and returns \texttt{true} whenever at least one of the possible substrings is correctly matched. So, unless there is some external reason for stating that the size of the comparison should be four letter substrings, the distinction between categorial rule-based and similarity-based comparison is a blurred one.

An additional difficulty with (i) is that it makes rule-based systems a special case of similar\-ity-based systems. This is because perfect matching will happen in similarity-based systems, which means that any similarity-based system can easily emulate a rule-based system.

Finally, partial matching has the problem that it is not easily computationally implementable. Systems which implement partial matching usually do some sort of statistical evaluation as in the model by \textcite{Albright.2003}, or decompose matches into smaller pieces. For example, the schema \textit{[kl\dots ɪNK]} can be simulated by doing smaller exact matches of its individual elements. A computer can be programmed to do matching based on estimated probabilities or confidence values, but in the end there is either a strong threshold, or some randomization process, neither of which really constitute partial matching.

The difficulty with (ii) is that, for the purpose of distinguishing between rules and similarity, it is a statement that is important from a psycholinguistic perspective, but not from a modelling perspective, as the authors admit (203--204):

\begin{quotation}
Rule-based reasoning implies rule-following: that a representation of a rule causally affects the behavior of the system and is not merely an apt summary description. Thus, only claims about rule-following are claims about cognitive architecture \autocite[203--204]{Hahn.1998}
\end{quotation}

Their point is that the distinction about abstractness is important if we are concerned about cognitive architecture, because from a purely descriptive perspective the distinction between rules and similarity breaks down. Thus, (ii) is more a statement about how speakers store and represent previously encountered items and the nature of those representations. Although the question of rich memory is an interesting and important one (see for example \cites{Bybee.2010, Kapatsinski.2014a, Port.2010a}, among many others), it is completely tangential to the issue at hand.

\citeauthor{Albright.2003}' (2003) attempt at distinguishing rules from analogy is even vaguer. The authors claim that the key difference between analogy and a rule is that rules represent \emph{structured similarity}, while analogy represents \emph{variegated similarity}. Structured similarity occurs when the similarity function is restricted by some structural property of the items it operates on, while variegated similarity occurs when it is not. If, for example, the similarity function can only look at the final syllable of a word, it is making use structured similarity. The toy example in \REF{structured-simil} illustrates the difference between variegated and structured similarity. The rule in (a) makes use of structured similarity while the rule in (b) makes use of variegated similarity. While both rules match the same segments, the rule in (a) makes use of phonological structure because it restricts the position of the similarity to the final syllable of the word. The rule in (b), on the other hand, matches any lexemes that contain the sequence \textit{/at/} in any position.

\begin{exe}
    \ex \label{structured-simil}
    \begin{xlist}
        \ex \textit{class--X} / \textit{.at\#}
        \ex \textit{class--X} / \textit{at}
    \end{xlist}
\end{exe}

This distinction is not very convincing, because it simple makes reference to a way of capturing similarity, which is mostly tangential to all other properties of analogical models. As \textcite[5]{Albright.2003} then point out, most connectionist models can infer structured similarity, which is why they do not consider these models as pure analogy. \textcite{Albright.2003} show that structured similarity seems to be a fundamental property of the linguistic systems they investigate, which they take to be support for rule rule-based models over analogical models. However, although it is true that some models ignore structure altogether, lumping connectionist models together with rule-based models based on whether phonological structure is at play or not draws an unnecessary ad-hoc line  between analogy and rules. From this perspective, none of the models I use for the case studies are purely analogical, since they heavily make use of structural constraints on the similarity function, but they certainly are nothing like typical rule-based models.

Finally, authors like \textcite{Pothos.2005}, working on analogy from a more general perspective and not specifically on linguistic systems, have also arrived at the conclusion that similarity (analogical) models and rule models are simply two extremes of the same gradient. For that reason, I will not attempt to draw clear distinctions between analogical and rule-based systems. I will employ neural networks for the case studies, but these models would work equally well with hand written rules or AM.
\is{analogy and rules|)}

\subsection{Mental representations vs grammatical relations}

Analogical models of grammar, and more generally, analogical accounts of grammatical phenomena are very often mixed in with discussions of mental storage, processing and psycholinguistic models (see for example \cite{Bybee.2010} and references therein). \textcite[419--420]{Eddington.2009}, for example, claims that ``[i]n contrast to rule systems, analogy assumes massive storage of previously experienced linguistic material'' and that ``linguistic cognition entails enormous amounts of storage and little processing''. This is not restricted to usage-based linguistics, for example \citegen{Gouskova.2015} model explicitly mentions of storage and processing by speakers (see Chapter \sectref{chap:hybrid} and the next section). The questions of language processing and mental representation of language are important, but we can study analogical relations in the lexicon independently of them.

Distinguishing between mental representations and grammatical descriptions is already commonplace in most formal approaches to grammar. \textcite[63--64]{Stump.2016}, for example, makes a distinction between the mental lexicon (the set of forms speakers actually store) and the stipulated lexicon (``the body of lexical information that is presupposed by the definition of a language's grammar'' (64)). Rich mental storage does not go against the idea of a stipulated lexicon, but mental storage of derived or inflected forms is a tangential question to the items that need to be in the stipulated lexicon. Whether speakers only stored inflected and derived high frequency forms \autocites{Pinker.2002, Ullman.2001, Ullman.2004}\footnote{This position is relatively common among formal linguists who accept that frequency plays a role in processing (see for example \citealt{Stump.2016} or \citealt{Muller.2014}), but it presents a problem with no solution as of yet: in these models, the only way of knowing whether a form has high frequency or low frequency, is to know its frequency. And the only way to know the frequency of a form is if said form has already been stored (\citealt{Bybee.2010}, but compare \citealt{Baayen.2013a}). The issue could be circumvented with more complex mental storage architectures which can model frequency learning without direct frequency representations \autocites{Baayen.2011, Baayen.2011a, Baayen.2010, Baayen.2013a}.} or (possibly) every single form they ever encounter \autocites{Baayen.2007, DeVaan.2007}, has no real impact on the number and nature of the items in the stipulated lexicon.

Nevertheless, the linguistic discourse on analogy has not been free from the confusion between mental representations and structural properties. The definitions usually given for analogical models make explicit reference to the mental lexicon, storage and actual speaker performance:

\begin{quotation}
  The analogical approach, on the other hand, deals with complex and simplex lexemes and the way they are connected to each other in the mental lexicon. It is argued that the formation of new complex lexemes is based on the paradigms of similar existing complex lexemes and their formal properties rather than on abstract rules. \autocite[1540]{Schlucker.2011}
\end{quotation}

or:

\begin{quotation}
  An important source of creativity and productivity in language that allows the expression of novel concepts and the description of novel situations is the ability to expand the schematic slots in constructions to fill them with novel lexical items, phrases or other constructions. Considerable evidence indicates that this process refers to specific sets of items that have been previously experienced and stored in memory. A number of researchers have used the term `analogy' to refer to the use of a novel item in an existing pattern, based on specific stored exemplars \autocite[57]{Bybee.2010}
\end{quotation}

Analogical relations do not require us to postulate mental storage or psychological processes and can be formulated independently of how speakers process language. The main point linguists working on analogy want to make is that analogy expresses a relation between word forms \autocite[11]{Becker.1990}.

While it is likely that speakers make use of some form of rich memory, and that analogy is closely linked to it, the model developed in this book does not require this assumption, but is compatible with it. The model I will develop in the following chapter is agnostic about these issues. The advantage of this approach is that we can avoid unnecessary debates and, most importantly, remove possible confounds.

\subsection{Summing up}

From a systemic perspective, there is not a real categorical distinction between schemata, computational systems and rules for modelling analogical classifiers. In the end, all these systems are used to find abstractions about the shape or meaning of lexemes and find similar clusters of lexemes which belong to the same class. The real difference appears when the question of learning and mental representation is put forward. Similarity-based systems, be they computational or schema-based, assume a rich memory model of language, where speakers store most of the items they encounter and actively use stored forms to process new ones. Rule-based models, on the other hand, make the assumption that rules are learned from very few items and stored independently and abstractly. The latter type of models do not usually assume rich memory.

If the main research goal is to address the question of how speakers process and represent linguistic structures, then the distinction between rule-based and similarity/analogy-based systems is important. However, as far as modelling is concern, we are simply talking about a matter of degree. Schema and rule-based  models explicitly write what the similarity relations between items must be, while AM and other computational models use mathematical objects to infer and store these similarity relations. Because in this book I do not explore the question of mental representation or acquisition, I will not argue for or against any particular implementation. The main claims of this book hold true for any of the approaches described above.\footnote{One possibly incompatible approach is Optimality Theory (OT). The problems that an OT model would face will become clear in the next chapter when I present the formal model of analogy.}

\section{Missing pieces}

Despite the great progress that has been made in terms of computational implementations of analogical classifiers (from now on also simply \textit{analogy}), as well as in the coverage of different phenomena, there are still a few conceptual issues that have been ignored and which require an answer. Broadly speaking, most work on analogy has been carried out within the cognitive linguistics and usage-based linguistics communities (the most recent exception being \textcite{Gouskova.2015}, who seems to mostly ignore work coming from these two communities). For better or worse, research on analogical classifiers has mainly focused on developing new and better computational models, as well as trying to find out what the limits of analogical classifiers are, by applying them to all kinds of phenomena. This, however, has come with a relative lack of attention to proper formalization of what analogical classifiers actually are and how they relate to grammar.

Some of the glaring problems were mentioned by \textcite{Wills.2012}. The authors argue that analogical classifiers (what they call ``categorization models'') suffer from not being explicit about their scope, and because they are fitted to each individual phenomenon, models are not consistent with the variables they work on. This is an important point. In the examples mentioned in the previous section, the analogical classifiers were built to deal with only one alternation. Each classifier looks at specific predictors relevant for each phenomenon, and it is strictly confined to that phenomenon. So far, there is no theory of how this restriction takes place or how it relates to the rest of the system. The main question missing and answer is given in:

\begin{exe}
    \ex \label{question} How do analogical processes (understood as analogical classifiers) interact with grammar and with each other?
\end{exe}

This is not trivial. There is, so far, no analogical model that can capture most of, let alone all, language domains. There are not even analogical systems that can capture most of a single domain. In other words, analogical classifiers are designed to capture specific phenomena within a well defined and limited domain, but they cannot capture the whole (or a sizable portion) of the morphology, phonology or syntax of a language. This basically means that even if we accept that a large number of phenomena in language require and are best accounted by, analogical systems, grammar (in the form of constructions, features, rules, etc.), still needs to take care of the rest. This also means that grammar needs to interact with analogy in a clearly defined way.

Unless the claim is put forward that \textit{all} grammatical phenomena in language can be accounted for with analogy, a formally well defined interface between analogy and grammar is required. This interface must make explicit what kinds of interactions we see between analogy and grammar, what the domains are and, importantly, where the limits of analogy lie.

The interactions between analogical classifiers are also poorly understood. Supposing that a language can have more than one phenomenon which is explained by an analogical classifier, it is not clear whether these two classifiers interact with each other and how. If a language organizes irregular verbs and nouns according to analogical classifiers, one would like to also know whether these classifiers are independent from each other and to what degree.

Another pressing issue relates to the targets of analogy, or the features analogy can and cannot see. \textcite[185]{Albright.2009} correctly points out that ``an adequate model of analogy must [\dots] be restrictive enough to explain why speakers generalize certain statistical properties of the data and not others''. This question has mostly been ignored. \textcite[54]{Bybee.2010} emphasizes that ``[m]ost analogical formations in language are based on semantic or phonological similarity with existing forms'', but acknowledges that

\begin{quotation}
  The problem faced in the full elaboration of such models, however, is in specifying the relevant features upon which similarity is measured.
  This is a pressing empirical problem. We need to ask, why are the final consonants of the strung verbs more important than the initial ones? \autocite[62]{Bybee.2010}
\end{quotation}

There have been a couple studies which have, only indirectly, dealt with some of these questions. In the generative literature most of the phenomena of phonologically conditioned allomorphy are dealt with either context rules, or OT, or sometimes simply just described but not really modelled (e.g. \citealt[119]{Rubach.2007}). In the usage-based literature the issue of analogy-grammar interaction is mostly ignored, or taken from granted. As far as I am aware, there have been no attempts at explicitly answering the question in \REF{question}, only a few informal approaches.

Probably the most explicit formulation of how analogy interacts with grammar is given by \textcite{Bybee.2015}. \citeauthor{Bybee.2015} suggest a model where analogy classifies lexical items according to whether they are compatible with different constructions or not. For \textcite{Bybee.2015}, a construction like: [X$_{\textsc{verb}}$-d] $\leftrightarrow$ [past(SEM(X))],\footnote{\textcite{Bybee.2015} use a slightly different notation, but the idea is the same.} is responsible for producing the past tense form of regular English verbs. What the analogical classifier does is simply decide which verbs can be combined with this construction.\footnote{A very similar model of analogy-grammar interaction is discussed by \textcite{Gouskova.2015}. Working in the framework of Distributed Morphology \autocite{Halle.1993}, \textcite{Gouskova.2015} propose a model for dealing with Russian diminutives based on more or less the same principles. See Chapter \sectref{chap:hybrid} for a discussion of this model.} However, \textcite{Bybee.2015} are not really explicit on how this happens or where. There are multiple alternatives: The analogical classifier could apply immediately whenever any new verb is learned and assign a feature to it specifying which inflectional construction it is compatible with, or it could apply every time a speaker wants to inflect said verb. It is also not clear how different constructions compete with each other. One could have a classifier which directly decides which constructions a lexical item is compatible with, or there could be individual classifiers for each constructions deciding whether some given lexical item is compatible or not.

All this being said, this book is mostly an attempt at formally implementing and testing the \citeauthor{Bybee.2015} proposal, where analogy and grammar are independent but closely interlinked with each other.

\section{Final considerations}

In this chapter I provided a short summary of some of the main different uses of analogy in linguistics. I presented single case analogies, proportional analogies and analogical classifiers. The main difference between single case analogy and analogical classifiers is that the former directly links forms to forms, while the latter uses an intermediate abstraction step that links forms to classes.

Analogical classifiers are of interest to both usage-based and formal linguists. Analogical classifiers are capable of capturing what has been seen as different processes (phonologically conditioned allomorphy, inflection classes, gender assignment, etc.) and treat them as a single phenomenon. There are several techniques used for implementing analogical classifiers (rules, schemata and computational implementations), and although superficially very different from each other, they are, at their core, very similar and often interchangeable.

Although there has been a considerable amount of research on analogical classifiers, there are still several questions pertaining to the interaction between analogical models and grammar. Answering these questions is crucial if we want to have a better understanding of exactly how much analogy can do and how much it cannot do. We want to avoid waiving away phenomena by simply invoking analogy as a magical solution, but we also want to avoid overly complicating grammatical analysis by trying to explain those aspects that analogical models can more easily capture.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
